\chapter*{Introducción}\label{chapter:introduction}
\addcontentsline{toc}{chapter}{Introducción}

\section*{Motivación}
Con el transcurso de los años, la inteligencia artificial ha tomado un papel importante en la sociedad, se han visto grandes avances en diferentes campos gracias a este, como lo es en la medicina ya sea para la detección de tumores, el desarrollo de diagnósticos, la creación de medicamentos; o por ejemplo en el procesamiento del lenguaje natural para automatizar procesos de atención al cliente, traducción, motores de búsquedas, etc. Incluso las grandes industrias se han ido alineando a estas técnicas, un ejemplo de esto es el servicio de aws \textbf{Machine Learning} que entre sus casos de usos está la recomendación personalizada a clientes, o la extracción y análisis automatizado de datos, identificar actividades fraudulentas, entre otros; así mismo podemos encontrar servicios en diferentes empresas que nos proveen de herramientas poderosas en el dominio de la inteligencia artificial y así potenciar la eficacia de nuestras tareas. Esto se ha logrado gracias al arduo trabajo de científicos en el tema, por lo que debido a su complejidad, con el tiempo ha surgido una comunidad con el objetivo de desarrollar métodos para resolver estos problemas prescindiendo del conocimiento de expertos, así las personas tienen a su alcance una herramienta que les permite encontrar soluciones sin necesariamente conocer las librerías que actúan. Estos métodos forman el área llamada AutoML o \textit{Auto Machine Learning}.\\

El AutoML es el proceso de automatizar la resolución de problemas de \textit{machine learning} en sus diferentes etapas, como el preproceso y limpieza de los datos, la selección de \textit{features}, selección del conjunto de algoritmos a utilizar, la configuración de los parámetros, la optimización de estos procesos, entre otros, con el objetivo de que se reduzca considerablemente el tiempo del humano en la investigación y el diseño de modelos de machine learning, mayormente se han enfocado en el estudio de \textit{supervised learning} aunque ya también se está explorando el contexto de \textit{unsupervised learning} \parencite{1} y \textit{semi supervised learning} \parencite{2}. La combinación de AutoML con metodologías de optimización para el despliegue de modelos de \textit{machine learning} es una herramienta trascendente para la democratización de la inteligencia artificial.\\

 Muchas compañías actualmente necesitan hacer uso de datos de diferentes dominios para mejorar sus productos, es difícil estudiarlos todos con la inteligencia humana, por lo que se ha hecho altamente necesario el uso de computadoras para automatizar predicciones y obtener las mejores resultados posibles. AutoML nos provee la mejor solución en base a su predicción y análisis.\\

Un modelo pre-entrenado es un modelo creado y entrenado con un conjunto de datos con el fin de resolver un problema similar, en vez de construir uno desde cero podemos usar un modelo pre-entrenado como punto de entrada. Existen varios beneficios de usar modelos preentrenados, reduce el tiempo computacional, te permite usar modelos en el estado del arte sin tener que entrenar uno de cero, cuando usamos un modelo preentrenado, lo entrenamos en un conjunto de datos específicos para la tarea que necesitamos realizar, este proceso es conocido como \textit{fine-tuning}, una técnica de entrenamiento poderosa. Una gran ventaja de hacer \textit{fine-tuning} a modelos preentrenados es fuerte rendimiento en cualquier momento, el uso de modelos preentrenados a menudo permite obtener órdenes de magnitudes de rendimiento más rápido que cuando se entrena un modelo desde cero. \\

Modelos preentrenados de grandes escalas como BERT \parencite{11} y GPT \parencite{12} han logrado recientemente un gran éxito y se han convertido en un hito en la inteligencia artificial. Debido a los sofisticados objetivos de preentrenamiento y los enormes parámetros del modelo, los modelos preentrenados a gran escala pueden capturar conocimiento de manera efectiva a partir de datos masivos etiquetados y no etiquetados. Al almacenar conocimiento en enormes parámetros y afinar tareas específicas, el rico conocimiento codificado implícitamente en parámetros enormes puede beneficiar una variedad de tareas posteriores, lo que se ha demostrado ampliamente a través de la verificación experimental y el análisis empírico \\

\section*{Antecedentes}
Ante la creciente demanda en la industria son muchas las librerías de código abierto que se han sumado al desarrollo de \textit{frameworks} para la automatización y solución de problemas de \textit{machine learning}, entre las más conocidas tenemos AutoWEKA \parencite{13}, AutoKeras \parencite{14}, Auto-sklearn \parencite{15}, Auto-PyTorch \parencite{16}; los cuales se han enfocado en la solución de modelos de aprendizaje supervisado.\\

Una segunda ola de AutoML inició en el 2010 con la introducción de modelos basados en Bayesian Optimization / Sequential Model-Based Optimization (SMBO) para la optimización de hiperparámetros y la selección de algoritmos \parencite{8} \parencite{3}. La idea intuitiva de estos métodos es usar un modelo sustituto para estimar las relaciones entre performance e hiperparámetros, y usar esta estimación para guiar el proceso de optimización a través de una función de adquisición. En el 2013 Thornton et al. introdujo Auto-WEKA, un método de AutoML basado en SMBO capaz de construir pipelines de clasificación en la popular plataforma WEKA \parencite{4}. Los autores definieron el problema de CASH, el cual se asemeja a la tarea de la selección del modelo completo. Auto-WEKA se basó en un método de SMBO llamado SMAC \parencite{3} con un estimador de random forest. Este método potenció la investigación en SMBO para AutoML tal que a día de hoy es el enfoque de optimización dominante en el campo. \\

También, metodologías alternativas no relacionadas a la formulación probabilística fueron propuestas. Por ejemplo, Rosales et al. desarrolló una metodología de AutoMl basada en optimización multi-objetivo y modelos sustitutos \parencite{5}. Un regresor fue usado para estimar el performance de soluciones de forma tal que solo las prometedoras fueran evaluadas con la función de objetivo de costo. Esta solución comparte el espíritu de SMBO pero se enfoca de una forma diferente. En los años siguientes soluciones basadas en SMBO han sido propuestas, más notablemente AutoSklearn \parencite{6}. Este método surgió en el contexto de desafíos académicos. AutoSklearn es basado en SMBO con la diferencia de que el proceso de búsqueda es inicializado con meta-learning lo que dirige la búsqueda a modelos prometedores. También este método genera un conjunto de soluciones exploradas durante el proceso de búsqueda. AutoSklearn ganó una serie de desafíos de AutoML, con gran margen en algunos escenarios \parencite{7}, incluso superaron a humanos que tenían como objetivo afinar un modelo. AutoSklearn fue publicado y a día de hoy es muy popular. \\

Los primeros esfuerzos de pre-entrenamiento están principalmente involucrados en \textit{transfer learning} \parencite{9}. El estudio de \textit{transfer learning} está fuertemente motivado por el hecho de que las personas pueden confiar en conocimiento previo para resolver nuevos problemas e incluso alcanzar mejores resultados. Más formalmente, \textit{transfer learning} tiene como objetivo capturar conocimiento importante de múltiples fuentes y aplicar conocimiento a una tarea destino. En \textit{transfer learning} la tarea fuente y la tarea destino quizás tengan dominios y configuraciones completamente diferentes, pero el conocimiento requerido para manejar estas tareas es consistente \parencite{10}. Por lo tanto es importante seleccionar un método viable para transferir el conocimiento de las tareas de origen a las tareas de destino. Con este fin se han propuesto varios métodos de pre-entrenamiento para trabajar como puente entre las tareas de origen y destino. Específicamente, estos métodos primero preentrenan modelos en los datos de múltiples tareas de origen para precodificar el conocimiento y luego transfieren el conocimiento precodificado para entrenar modelos para tareas de destino.


\section*{Problema}
AutoGOAL \parencite{71} es una biblioteca de Python para encontrar automáticamente la mejor manera de resolver una tarea determinada. Se ha diseñado principalmente para el AutoML, pero se puede usar en cualquier escenario en el que tenga varias formas posibles de resolver una tarea determinada.

Técnicamente hablando, AutoGOAL es un marco para la síntesis de programas, es decir, encontrar el mejor programa para resolver un problema dado, siempre que el usuario pueda describir el espacio de todos los programas posibles. AutoGOAL proporciona un conjunto de componentes de bajo nivel para definir diferentes espacios y buscar eficientemente en ellos. En el contexto específico del \textit{machine learning}, AutoGOAL también proporciona componentes de alto nivel que se pueden usar como una caja negra en casi cualquier tipo de problema y formato de conjunto de datos.

Actualmente no existe mucha información acerca del uso de modelos pre-entrenados en los distintos métodos de AutoML, por lo que añadirlos al dominio de algoritmos de AutoML puede generar excelentes resultados en menor costo temporal y computacional, así como sustituir algunos modelos que son entrenados desde cero.

\section*{Objetivos}
El objetivo de esta tesis es tomar modelos preentrenados en diferentes dominios relativamente importantes e incorporarles técnicas de fine-tuning de forma tal que puedan ser usados como parte de pipelines en AutoGoal.\\

\begin{itemize}

\item Añadir modelos preentrenados al conjunto de algoritmos de AutoGoal.
\item Ser competitivo en el estado del arte con los problemas clásicos de AutoML usando estos modelos preentrenados.
\item Poder comparar el uso de algoritmos ya existentes en AutoGoal con el uso de modelos preentrenados afinados para resolver tareas similares.

\end{itemize}

\section*{Estructura de la tesis}

